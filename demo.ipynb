{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "  def __init__(self, env, path=None):\n",
    "    self.env=env \n",
    "    self.state_shape=env.observation_space.shape # the state space\n",
    "    self.action_shape=env.action_space.n # the action space\n",
    "    self.gamma=0.99 # decay rate of past observations\n",
    "    self.alpha=1e-4 # learning rate of gradient\n",
    "    self.learning_rate=0.01 # learning of deep learning model\n",
    "    \n",
    "    if not path:\n",
    "      print('Policy network')\n",
    "      self.model=self.build_policy_network() #build model\n",
    "    else:\n",
    "      self.model=self.load_model(path) #import model\n",
    "\n",
    "    # record observations\n",
    "    self.states=[]\n",
    "    self.gradients=[] \n",
    "    self.rewards=[]\n",
    "    self.probs=[]\n",
    "    self.discounted_rewards=[]\n",
    "    self.total_rewards=[]\n",
    "    \n",
    "    \n",
    "\n",
    "  def build_policy_network(self):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(24, input_shape=self.state_shape, activation=\"relu\"))\n",
    "    model.add(Dense(12, activation=\"relu\"))\n",
    "    model.add(Dense(self.action_shape, activation=\"softmax\")) \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=self.learning_rate))\n",
    "    keras2ascii(model)\n",
    "    return model\n",
    "\n",
    "  def hot_encode_action(self, action):\n",
    "    action_encoded=np.zeros(self.action_shape)\n",
    "    action_encoded[action]=1\n",
    "    return action_encoded\n",
    "  \n",
    "  def remember(self, state, action, action_prob, reward):\n",
    "    encoded_action=self.hot_encode_action(action)\n",
    "    self.gradients.append(encoded_action-action_prob)\n",
    "    self.states.append(state)\n",
    "    self.rewards.append(reward)\n",
    "    self.probs.append(action_prob)\n",
    "\n",
    "  def compute_action(self, state):\n",
    "\n",
    "    # transform state\n",
    "    state=state.reshape([1, state.shape[0]])\n",
    "    # get action probably\n",
    "    action_probability_distribution=self.model.predict(state, verbose=0).flatten()\n",
    "    # norm action probability distribution\n",
    "    action_probability_distribution/=np.sum(action_probability_distribution)\n",
    "    \n",
    "    # sample action\n",
    "    action=np.random.choice(self.action_shape,1,\n",
    "                            p=action_probability_distribution)[0]\n",
    "\n",
    "    return action, action_probability_distribution\n",
    "\n",
    "\n",
    "  def get_discounted_rewards(self, rewards): \n",
    "   \n",
    "    discounted_rewards=[]\n",
    "    cumulative_total_return=0\n",
    "    # iterate the rewards backwards and calc the total return \n",
    "    for reward in rewards[::-1]:      \n",
    "      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n",
    "      discounted_rewards.insert(0, cumulative_total_return)\n",
    "\n",
    "    # normalize discounted rewards\n",
    "    mean_rewards=np.mean(discounted_rewards)\n",
    "    std_rewards=np.std(discounted_rewards)\n",
    "    norm_discounted_rewards=(discounted_rewards-\n",
    "                          mean_rewards)/(std_rewards+1e-7) # avoiding zero div\n",
    "    \n",
    "    return norm_discounted_rewards\n",
    "\n",
    "  def train_policy_network(self):\n",
    "       \n",
    "    # get X_train\n",
    "    states=np.vstack(self.states)\n",
    "\n",
    "    # get y_train\n",
    "    gradients=np.vstack(self.gradients)\n",
    "    rewards=np.vstack(self.rewards)\n",
    "    discounted_rewards=self.get_discounted_rewards(rewards)\n",
    "    gradients*=discounted_rewards\n",
    "    y_train = self.alpha*np.vstack([gradients])+self.probs\n",
    "    #y_train = gradients\n",
    "    history=self.model.train_on_batch(states, y_train) #Batch size depends on outcome of episode length, train_on_batch() has no fixed batch size (ex. 128 ,256, ...)\n",
    "    \n",
    "    self.states, self.probs, self.gradients, self.rewards=[], [], [], []\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "  def train(self, episodes):\n",
    "     \n",
    "    env=self.env\n",
    "    total_rewards=np.zeros(episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "      # each episode is a new game env\n",
    "      state=env.reset()[0]\n",
    "      done=False          \n",
    "      episode_reward=0 #record episode reward\n",
    "      \n",
    "      while not done:\n",
    "        # play an action and record the game state & reward per episode\n",
    "        action, prob=self.compute_action(state)\n",
    "        next_state, reward, done, _, _=env.step(action)\n",
    "        self.remember(state, action, prob, reward)\n",
    "        state=next_state\n",
    "        episode_reward+=reward\n",
    "\n",
    "        #if episode%render_n==0: ## render env to visualize.\n",
    "        env.render()\n",
    "        if done:\n",
    "          # update policy \n",
    "            history=self.train_policy_network()\n",
    "\n",
    "      total_rewards[episode]=episode_reward\n",
    "      print('\\n episode = ',episode, 'reward = ',episode_reward)\n",
    "      \n",
    "    self.total_rewards=total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy network\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####           4\n",
      "               Dense   XXXXX -------------------       120    26.9%\n",
      "                relu   #####          24\n",
      "               Dense   XXXXX -------------------       300    67.3%\n",
      "                relu   #####          12\n",
      "               Dense   XXXXX -------------------        26     5.8%\n",
      "             softmax   #####           2\n",
      "\n",
      " episode =  0 reward =  10.0\n",
      "\n",
      " episode =  1 reward =  13.0\n",
      "\n",
      " episode =  2 reward =  24.0\n",
      "\n",
      " episode =  3 reward =  11.0\n",
      "\n",
      " episode =  4 reward =  22.0\n",
      "\n",
      " episode =  5 reward =  10.0\n",
      "\n",
      " episode =  6 reward =  18.0\n",
      "\n",
      " episode =  7 reward =  13.0\n",
      "\n",
      " episode =  8 reward =  10.0\n",
      "\n",
      " episode =  9 reward =  10.0\n",
      "\n",
      " episode =  10 reward =  26.0\n",
      "\n",
      " episode =  11 reward =  12.0\n",
      "\n",
      " episode =  12 reward =  17.0\n",
      "\n",
      " episode =  13 reward =  39.0\n",
      "\n",
      " episode =  14 reward =  15.0\n",
      "\n",
      " episode =  15 reward =  20.0\n",
      "\n",
      " episode =  16 reward =  13.0\n",
      "\n",
      " episode =  17 reward =  40.0\n",
      "\n",
      " episode =  18 reward =  77.0\n",
      "\n",
      " episode =  19 reward =  9.0\n",
      "\n",
      " episode =  20 reward =  11.0\n",
      "\n",
      " episode =  21 reward =  20.0\n",
      "\n",
      " episode =  22 reward =  14.0\n",
      "\n",
      " episode =  23 reward =  20.0\n",
      "\n",
      " episode =  24 reward =  12.0\n",
      "\n",
      " episode =  25 reward =  12.0\n",
      "\n",
      " episode =  26 reward =  32.0\n",
      "\n",
      " episode =  27 reward =  16.0\n",
      "\n",
      " episode =  28 reward =  10.0\n",
      "\n",
      " episode =  29 reward =  14.0\n",
      "\n",
      " episode =  30 reward =  12.0\n",
      "\n",
      " episode =  31 reward =  38.0\n",
      "\n",
      " episode =  32 reward =  18.0\n",
      "\n",
      " episode =  33 reward =  12.0\n",
      "\n",
      " episode =  34 reward =  21.0\n",
      "\n",
      " episode =  35 reward =  38.0\n",
      "\n",
      " episode =  36 reward =  11.0\n",
      "\n",
      " episode =  37 reward =  12.0\n",
      "\n",
      " episode =  38 reward =  15.0\n",
      "\n",
      " episode =  39 reward =  13.0\n",
      "\n",
      " episode =  40 reward =  14.0\n",
      "\n",
      " episode =  41 reward =  11.0\n",
      "\n",
      " episode =  42 reward =  20.0\n",
      "\n",
      " episode =  43 reward =  25.0\n",
      "\n",
      " episode =  44 reward =  23.0\n",
      "\n",
      " episode =  45 reward =  37.0\n",
      "\n",
      " episode =  46 reward =  9.0\n",
      "\n",
      " episode =  47 reward =  32.0\n",
      "\n",
      " episode =  48 reward =  38.0\n",
      "\n",
      " episode =  49 reward =  14.0\n",
      "\n",
      " episode =  50 reward =  13.0\n",
      "\n",
      " episode =  51 reward =  20.0\n",
      "\n",
      " episode =  52 reward =  11.0\n",
      "\n",
      " episode =  53 reward =  11.0\n",
      "\n",
      " episode =  54 reward =  30.0\n",
      "\n",
      " episode =  55 reward =  11.0\n",
      "\n",
      " episode =  56 reward =  10.0\n",
      "\n",
      " episode =  57 reward =  27.0\n",
      "\n",
      " episode =  58 reward =  38.0\n",
      "\n",
      " episode =  59 reward =  14.0\n",
      "\n",
      " episode =  60 reward =  13.0\n",
      "\n",
      " episode =  61 reward =  22.0\n",
      "\n",
      " episode =  62 reward =  13.0\n",
      "\n",
      " episode =  63 reward =  27.0\n",
      "\n",
      " episode =  64 reward =  15.0\n",
      "\n",
      " episode =  65 reward =  19.0\n",
      "\n",
      " episode =  66 reward =  19.0\n",
      "\n",
      " episode =  67 reward =  13.0\n",
      "\n",
      " episode =  68 reward =  24.0\n",
      "\n",
      " episode =  69 reward =  16.0\n",
      "\n",
      " episode =  70 reward =  21.0\n",
      "\n",
      " episode =  71 reward =  18.0\n",
      "\n",
      " episode =  72 reward =  37.0\n",
      "\n",
      " episode =  73 reward =  11.0\n",
      "\n",
      " episode =  74 reward =  27.0\n",
      "\n",
      " episode =  75 reward =  24.0\n",
      "\n",
      " episode =  76 reward =  39.0\n",
      "\n",
      " episode =  77 reward =  16.0\n",
      "\n",
      " episode =  78 reward =  19.0\n",
      "\n",
      " episode =  79 reward =  19.0\n",
      "\n",
      " episode =  80 reward =  36.0\n",
      "\n",
      " episode =  81 reward =  44.0\n",
      "\n",
      " episode =  82 reward =  28.0\n",
      "\n",
      " episode =  83 reward =  58.0\n",
      "\n",
      " episode =  84 reward =  24.0\n",
      "\n",
      " episode =  85 reward =  16.0\n",
      "\n",
      " episode =  86 reward =  21.0\n",
      "\n",
      " episode =  87 reward =  9.0\n",
      "\n",
      " episode =  88 reward =  62.0\n",
      "\n",
      " episode =  89 reward =  21.0\n",
      "\n",
      " episode =  90 reward =  24.0\n",
      "\n",
      " episode =  91 reward =  25.0\n",
      "\n",
      " episode =  92 reward =  50.0\n",
      "\n",
      " episode =  93 reward =  16.0\n",
      "\n",
      " episode =  94 reward =  31.0\n",
      "\n",
      " episode =  95 reward =  37.0\n",
      "\n",
      " episode =  96 reward =  28.0\n",
      "\n",
      " episode =  97 reward =  41.0\n",
      "\n",
      " episode =  98 reward =  63.0\n",
      "\n",
      " episode =  99 reward =  34.0\n",
      "\n",
      " episode =  100 reward =  54.0\n",
      "\n",
      " episode =  101 reward =  38.0\n",
      "\n",
      " episode =  102 reward =  59.0\n",
      "\n",
      " episode =  103 reward =  99.0\n",
      "\n",
      " episode =  104 reward =  24.0\n",
      "\n",
      " episode =  105 reward =  41.0\n",
      "\n",
      " episode =  106 reward =  34.0\n",
      "\n",
      " episode =  107 reward =  49.0\n",
      "\n",
      " episode =  108 reward =  52.0\n",
      "\n",
      " episode =  109 reward =  66.0\n",
      "\n",
      " episode =  110 reward =  58.0\n",
      "\n",
      " episode =  111 reward =  54.0\n",
      "\n",
      " episode =  112 reward =  68.0\n",
      "\n",
      " episode =  113 reward =  46.0\n",
      "\n",
      " episode =  114 reward =  18.0\n",
      "\n",
      " episode =  115 reward =  49.0\n",
      "\n",
      " episode =  116 reward =  61.0\n",
      "\n",
      " episode =  117 reward =  29.0\n",
      "\n",
      " episode =  118 reward =  30.0\n",
      "\n",
      " episode =  119 reward =  36.0\n",
      "\n",
      " episode =  120 reward =  47.0\n",
      "\n",
      " episode =  121 reward =  25.0\n",
      "\n",
      " episode =  122 reward =  60.0\n",
      "\n",
      " episode =  123 reward =  70.0\n",
      "\n",
      " episode =  124 reward =  39.0\n",
      "\n",
      " episode =  125 reward =  132.0\n",
      "\n",
      " episode =  126 reward =  95.0\n",
      "\n",
      " episode =  127 reward =  25.0\n",
      "\n",
      " episode =  128 reward =  53.0\n",
      "\n",
      " episode =  129 reward =  60.0\n",
      "\n",
      " episode =  130 reward =  34.0\n",
      "\n",
      " episode =  131 reward =  81.0\n",
      "\n",
      " episode =  132 reward =  51.0\n",
      "\n",
      " episode =  133 reward =  31.0\n",
      "\n",
      " episode =  134 reward =  51.0\n",
      "\n",
      " episode =  135 reward =  98.0\n",
      "\n",
      " episode =  136 reward =  24.0\n",
      "\n",
      " episode =  137 reward =  74.0\n",
      "\n",
      " episode =  138 reward =  56.0\n",
      "\n",
      " episode =  139 reward =  24.0\n",
      "\n",
      " episode =  140 reward =  51.0\n",
      "\n",
      " episode =  141 reward =  43.0\n",
      "\n",
      " episode =  142 reward =  46.0\n",
      "\n",
      " episode =  143 reward =  58.0\n",
      "\n",
      " episode =  144 reward =  40.0\n",
      "\n",
      " episode =  145 reward =  30.0\n",
      "\n",
      " episode =  146 reward =  83.0\n"
     ]
    }
   ],
   "source": [
    "ENV=\"CartPole-v1\"\n",
    "\n",
    "N_EPISODES=500\n",
    "\n",
    "# set the env\n",
    "env=gym.make(ENV) # env to import\n",
    "env.reset() # reset to env\n",
    "\n",
    "\n",
    "Agent = REINFORCE(env)\n",
    "\n",
    "Agent.train(N_EPISODES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
