{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "  def __init__(self, env, path=None):\n",
    "    self.env=env \n",
    "    self.state_shape=env.observation_space.shape # the state space\n",
    "    self.action_shape=env.action_space.n # the action space\n",
    "    self.gamma=0.99 # decay rate of past observations\n",
    "    self.alpha=1e-4 # learning rate of gradient\n",
    "    self.learning_rate=0.01 # learning of deep learning model\n",
    "    \n",
    "    if not path:\n",
    "      print('Policy network')\n",
    "      self.model=self.build_policy_network() #build model\n",
    "    else:\n",
    "      self.model=self.load_model(path) #import model\n",
    "\n",
    "    # record observations\n",
    "    self.states=[]\n",
    "    self.gradients=[] \n",
    "    self.rewards=[]\n",
    "    self.probs=[]\n",
    "    self.discounted_rewards=[]\n",
    "    self.total_rewards=[]\n",
    "    \n",
    "    \n",
    "\n",
    "  def build_policy_network(self):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(24, input_shape=self.state_shape, activation=\"relu\"))\n",
    "    model.add(Dense(12, activation=\"relu\"))\n",
    "    model.add(Dense(self.action_shape, activation=\"softmax\")) \n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "            optimizer=Adam(learning_rate=self.learning_rate))\n",
    "    keras2ascii(model)\n",
    "    return model\n",
    "\n",
    "  def hot_encode_action(self, action):\n",
    "    action_encoded=np.zeros(self.action_shape)\n",
    "    action_encoded[action]=1\n",
    "    return action_encoded\n",
    "  \n",
    "  def remember(self, state, action, action_prob, reward):\n",
    "    encoded_action=self.hot_encode_action(action)\n",
    "    self.gradients.append(encoded_action-action_prob)\n",
    "    self.states.append(state)\n",
    "    self.rewards.append(reward)\n",
    "    self.probs.append(action_prob)\n",
    "\n",
    "  def compute_action(self, state):\n",
    "\n",
    "    # transform state\n",
    "    state=state.reshape([1, state.shape[0]])\n",
    "    # get action probably\n",
    "    action_probability_distribution=self.model.predict(state, verbose=0).flatten()\n",
    "    # norm action probability distribution\n",
    "    action_probability_distribution/=np.sum(action_probability_distribution)\n",
    "    \n",
    "    # sample action\n",
    "    action=np.random.choice(self.action_shape,1,\n",
    "                            p=action_probability_distribution)[0]\n",
    "\n",
    "    return action, action_probability_distribution\n",
    "\n",
    "\n",
    "  def get_discounted_rewards(self, rewards): \n",
    "   \n",
    "    discounted_rewards=[]\n",
    "    cumulative_total_return=0\n",
    "    # iterate the rewards backwards and calc the total return \n",
    "    for reward in rewards[::-1]:      \n",
    "      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n",
    "      discounted_rewards.insert(0, cumulative_total_return)\n",
    "\n",
    "    # normalize discounted rewards\n",
    "    mean_rewards=np.mean(discounted_rewards)\n",
    "    std_rewards=np.std(discounted_rewards)\n",
    "    norm_discounted_rewards=(discounted_rewards-\n",
    "                          mean_rewards)/(std_rewards+1e-7) # avoiding zero div\n",
    "    \n",
    "    return norm_discounted_rewards\n",
    "\n",
    "  def train_policy_network(self):\n",
    "       \n",
    "    # get X_train\n",
    "    states=np.vstack(self.states)\n",
    "\n",
    "    # get y_train\n",
    "    gradients=np.vstack(self.gradients)\n",
    "    rewards=np.vstack(self.rewards)\n",
    "    discounted_rewards=self.get_discounted_rewards(rewards)\n",
    "    gradients*=discounted_rewards\n",
    "    y_train = self.alpha*np.vstack([gradients])+self.probs\n",
    "    #y_train = gradients\n",
    "    history=self.model.train_on_batch(states, y_train) #Batch size depends on outcome of episode length, train_on_batch() has no fixed batch size (ex. 128 ,256, ...)\n",
    "    \n",
    "    self.states, self.probs, self.gradients, self.rewards=[], [], [], []\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "  def train(self, episodes):\n",
    "     \n",
    "    env=self.env\n",
    "    total_rewards=np.zeros(episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "      # each episode is a new game env\n",
    "      state=env.reset()[0]\n",
    "      done=False          \n",
    "      episode_reward=0 #record episode reward\n",
    "      \n",
    "      while not done:\n",
    "        # play an action and record the game state & reward per episode\n",
    "        action, prob=self.compute_action(state)\n",
    "        next_state, reward, done, _, _=env.step(action)\n",
    "        self.remember(state, action, prob, reward)\n",
    "        state=next_state\n",
    "        episode_reward+=reward\n",
    "\n",
    "        #if episode%render_n==0: ## render env to visualize.\n",
    "        env.render()\n",
    "        if done:\n",
    "          # update policy \n",
    "            history=self.train_policy_network()\n",
    "\n",
    "      total_rewards[episode]=episode_reward\n",
    "      print('\\n episode = ',episode, 'reward = ',episode_reward)\n",
    "      \n",
    "    self.total_rewards=total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy network\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####           4\n",
      "               Dense   XXXXX -------------------       120    26.9%\n",
      "                relu   #####          24\n",
      "               Dense   XXXXX -------------------       300    67.3%\n",
      "                relu   #####          12\n",
      "               Dense   XXXXX -------------------        26     5.8%\n",
      "             softmax   #####           2\n",
      "\n",
      " episode =  0 reward =  10.0\n",
      "\n",
      " episode =  1 reward =  13.0\n",
      "\n",
      " episode =  2 reward =  24.0\n",
      "\n",
      " episode =  3 reward =  11.0\n",
      "\n",
      " episode =  4 reward =  22.0\n",
      "\n",
      " episode =  5 reward =  10.0\n",
      "\n",
      " episode =  6 reward =  18.0\n",
      "\n",
      " episode =  7 reward =  13.0\n",
      "\n",
      " episode =  8 reward =  10.0\n",
      "\n",
      " episode =  9 reward =  10.0\n",
      "\n",
      " episode =  10 reward =  26.0\n",
      "\n",
      " episode =  11 reward =  12.0\n",
      "\n",
      " episode =  12 reward =  17.0\n",
      "\n",
      " episode =  13 reward =  39.0\n",
      "\n",
      " episode =  14 reward =  15.0\n",
      "\n",
      " episode =  15 reward =  20.0\n",
      "\n",
      " episode =  16 reward =  13.0\n",
      "\n",
      " episode =  17 reward =  40.0\n",
      "\n",
      " episode =  18 reward =  77.0\n",
      "\n",
      " episode =  19 reward =  9.0\n",
      "\n",
      " episode =  20 reward =  11.0\n",
      "\n",
      " episode =  21 reward =  20.0\n",
      "\n",
      " episode =  22 reward =  14.0\n",
      "\n",
      " episode =  23 reward =  20.0\n",
      "\n",
      " episode =  24 reward =  12.0\n",
      "\n",
      " episode =  25 reward =  12.0\n",
      "\n",
      " episode =  26 reward =  32.0\n",
      "\n",
      " episode =  27 reward =  16.0\n",
      "\n",
      " episode =  28 reward =  10.0\n",
      "\n",
      " episode =  29 reward =  14.0\n",
      "\n",
      " episode =  30 reward =  12.0\n",
      "\n",
      " episode =  31 reward =  38.0\n",
      "\n",
      " episode =  32 reward =  18.0\n",
      "\n",
      " episode =  33 reward =  12.0\n",
      "\n",
      " episode =  34 reward =  21.0\n",
      "\n",
      " episode =  35 reward =  38.0\n",
      "\n",
      " episode =  36 reward =  11.0\n",
      "\n",
      " episode =  37 reward =  12.0\n",
      "\n",
      " episode =  38 reward =  15.0\n",
      "\n",
      " episode =  39 reward =  13.0\n",
      "\n",
      " episode =  40 reward =  14.0\n",
      "\n",
      " episode =  41 reward =  11.0\n",
      "\n",
      " episode =  42 reward =  20.0\n",
      "\n",
      " episode =  43 reward =  25.0\n",
      "\n",
      " episode =  44 reward =  23.0\n",
      "\n",
      " episode =  45 reward =  37.0\n",
      "\n",
      " episode =  46 reward =  9.0\n",
      "\n",
      " episode =  47 reward =  32.0\n",
      "\n",
      " episode =  48 reward =  38.0\n",
      "\n",
      " episode =  49 reward =  14.0\n",
      "\n",
      " episode =  50 reward =  13.0\n",
      "\n",
      " episode =  51 reward =  20.0\n",
      "\n",
      " episode =  52 reward =  11.0\n",
      "\n",
      " episode =  53 reward =  11.0\n",
      "\n",
      " episode =  54 reward =  30.0\n",
      "\n",
      " episode =  55 reward =  11.0\n",
      "\n",
      " episode =  56 reward =  10.0\n",
      "\n",
      " episode =  57 reward =  27.0\n",
      "\n",
      " episode =  58 reward =  38.0\n",
      "\n",
      " episode =  59 reward =  14.0\n",
      "\n",
      " episode =  60 reward =  13.0\n",
      "\n",
      " episode =  61 reward =  22.0\n",
      "\n",
      " episode =  62 reward =  13.0\n",
      "\n",
      " episode =  63 reward =  27.0\n",
      "\n",
      " episode =  64 reward =  15.0\n",
      "\n",
      " episode =  65 reward =  19.0\n",
      "\n",
      " episode =  66 reward =  19.0\n",
      "\n",
      " episode =  67 reward =  13.0\n",
      "\n",
      " episode =  68 reward =  24.0\n",
      "\n",
      " episode =  69 reward =  16.0\n",
      "\n",
      " episode =  70 reward =  21.0\n",
      "\n",
      " episode =  71 reward =  18.0\n",
      "\n",
      " episode =  72 reward =  37.0\n",
      "\n",
      " episode =  73 reward =  11.0\n",
      "\n",
      " episode =  74 reward =  27.0\n",
      "\n",
      " episode =  75 reward =  24.0\n",
      "\n",
      " episode =  76 reward =  39.0\n",
      "\n",
      " episode =  77 reward =  16.0\n",
      "\n",
      " episode =  78 reward =  19.0\n",
      "\n",
      " episode =  79 reward =  19.0\n",
      "\n",
      " episode =  80 reward =  36.0\n",
      "\n",
      " episode =  81 reward =  44.0\n",
      "\n",
      " episode =  82 reward =  28.0\n",
      "\n",
      " episode =  83 reward =  58.0\n",
      "\n",
      " episode =  84 reward =  24.0\n",
      "\n",
      " episode =  85 reward =  16.0\n",
      "\n",
      " episode =  86 reward =  21.0\n",
      "\n",
      " episode =  87 reward =  9.0\n",
      "\n",
      " episode =  88 reward =  62.0\n",
      "\n",
      " episode =  89 reward =  21.0\n",
      "\n",
      " episode =  90 reward =  24.0\n",
      "\n",
      " episode =  91 reward =  25.0\n",
      "\n",
      " episode =  92 reward =  50.0\n",
      "\n",
      " episode =  93 reward =  16.0\n",
      "\n",
      " episode =  94 reward =  31.0\n",
      "\n",
      " episode =  95 reward =  37.0\n",
      "\n",
      " episode =  96 reward =  28.0\n",
      "\n",
      " episode =  97 reward =  41.0\n",
      "\n",
      " episode =  98 reward =  63.0\n",
      "\n",
      " episode =  99 reward =  34.0\n",
      "\n",
      " episode =  100 reward =  54.0\n",
      "\n",
      " episode =  101 reward =  38.0\n",
      "\n",
      " episode =  102 reward =  59.0\n",
      "\n",
      " episode =  103 reward =  99.0\n",
      "\n",
      " episode =  104 reward =  24.0\n",
      "\n",
      " episode =  105 reward =  41.0\n",
      "\n",
      " episode =  106 reward =  34.0\n",
      "\n",
      " episode =  107 reward =  49.0\n",
      "\n",
      " episode =  108 reward =  52.0\n",
      "\n",
      " episode =  109 reward =  66.0\n",
      "\n",
      " episode =  110 reward =  58.0\n",
      "\n",
      " episode =  111 reward =  54.0\n",
      "\n",
      " episode =  112 reward =  68.0\n",
      "\n",
      " episode =  113 reward =  46.0\n",
      "\n",
      " episode =  114 reward =  18.0\n",
      "\n",
      " episode =  115 reward =  49.0\n",
      "\n",
      " episode =  116 reward =  61.0\n",
      "\n",
      " episode =  117 reward =  29.0\n",
      "\n",
      " episode =  118 reward =  30.0\n",
      "\n",
      " episode =  119 reward =  36.0\n",
      "\n",
      " episode =  120 reward =  47.0\n",
      "\n",
      " episode =  121 reward =  25.0\n",
      "\n",
      " episode =  122 reward =  60.0\n",
      "\n",
      " episode =  123 reward =  70.0\n",
      "\n",
      " episode =  124 reward =  39.0\n",
      "\n",
      " episode =  125 reward =  132.0\n",
      "\n",
      " episode =  126 reward =  95.0\n",
      "\n",
      " episode =  127 reward =  25.0\n",
      "\n",
      " episode =  128 reward =  53.0\n",
      "\n",
      " episode =  129 reward =  60.0\n",
      "\n",
      " episode =  130 reward =  34.0\n",
      "\n",
      " episode =  131 reward =  81.0\n",
      "\n",
      " episode =  132 reward =  51.0\n",
      "\n",
      " episode =  133 reward =  31.0\n",
      "\n",
      " episode =  134 reward =  51.0\n",
      "\n",
      " episode =  135 reward =  98.0\n",
      "\n",
      " episode =  136 reward =  24.0\n",
      "\n",
      " episode =  137 reward =  74.0\n",
      "\n",
      " episode =  138 reward =  56.0\n",
      "\n",
      " episode =  139 reward =  24.0\n",
      "\n",
      " episode =  140 reward =  51.0\n",
      "\n",
      " episode =  141 reward =  43.0\n",
      "\n",
      " episode =  142 reward =  46.0\n",
      "\n",
      " episode =  143 reward =  58.0\n",
      "\n",
      " episode =  144 reward =  40.0\n",
      "\n",
      " episode =  145 reward =  30.0\n",
      "\n",
      " episode =  146 reward =  83.0\n",
      "\n",
      " episode =  147 reward =  46.0\n",
      "\n",
      " episode =  148 reward =  33.0\n",
      "\n",
      " episode =  149 reward =  34.0\n",
      "\n",
      " episode =  150 reward =  28.0\n",
      "\n",
      " episode =  151 reward =  32.0\n",
      "\n",
      " episode =  152 reward =  38.0\n",
      "\n",
      " episode =  153 reward =  66.0\n",
      "\n",
      " episode =  154 reward =  59.0\n",
      "\n",
      " episode =  155 reward =  66.0\n",
      "\n",
      " episode =  156 reward =  31.0\n",
      "\n",
      " episode =  157 reward =  84.0\n",
      "\n",
      " episode =  158 reward =  38.0\n",
      "\n",
      " episode =  159 reward =  39.0\n",
      "\n",
      " episode =  160 reward =  108.0\n",
      "\n",
      " episode =  161 reward =  38.0\n",
      "\n",
      " episode =  162 reward =  82.0\n",
      "\n",
      " episode =  163 reward =  71.0\n",
      "\n",
      " episode =  164 reward =  89.0\n",
      "\n",
      " episode =  165 reward =  67.0\n",
      "\n",
      " episode =  166 reward =  143.0\n",
      "\n",
      " episode =  167 reward =  103.0\n",
      "\n",
      " episode =  168 reward =  89.0\n",
      "\n",
      " episode =  169 reward =  91.0\n",
      "\n",
      " episode =  170 reward =  149.0\n",
      "\n",
      " episode =  171 reward =  159.0\n",
      "\n",
      " episode =  172 reward =  87.0\n",
      "\n",
      " episode =  173 reward =  60.0\n",
      "\n",
      " episode =  174 reward =  73.0\n",
      "\n",
      " episode =  175 reward =  82.0\n",
      "\n",
      " episode =  176 reward =  97.0\n",
      "\n",
      " episode =  177 reward =  92.0\n",
      "\n",
      " episode =  178 reward =  108.0\n",
      "\n",
      " episode =  179 reward =  136.0\n",
      "\n",
      " episode =  180 reward =  74.0\n",
      "\n",
      " episode =  181 reward =  157.0\n",
      "\n",
      " episode =  182 reward =  96.0\n",
      "\n",
      " episode =  183 reward =  55.0\n",
      "\n",
      " episode =  184 reward =  290.0\n",
      "\n",
      " episode =  185 reward =  165.0\n",
      "\n",
      " episode =  186 reward =  158.0\n",
      "\n",
      " episode =  187 reward =  176.0\n",
      "\n",
      " episode =  188 reward =  298.0\n",
      "\n",
      " episode =  189 reward =  234.0\n",
      "\n",
      " episode =  190 reward =  186.0\n",
      "\n",
      " episode =  191 reward =  147.0\n",
      "\n",
      " episode =  192 reward =  309.0\n",
      "\n",
      " episode =  193 reward =  124.0\n",
      "\n",
      " episode =  194 reward =  168.0\n",
      "\n",
      " episode =  195 reward =  117.0\n",
      "\n",
      " episode =  196 reward =  102.0\n",
      "\n",
      " episode =  197 reward =  159.0\n",
      "\n",
      " episode =  198 reward =  142.0\n",
      "\n",
      " episode =  199 reward =  62.0\n",
      "\n",
      " episode =  200 reward =  124.0\n",
      "\n",
      " episode =  201 reward =  44.0\n",
      "\n",
      " episode =  202 reward =  158.0\n",
      "\n",
      " episode =  203 reward =  254.0\n",
      "\n",
      " episode =  204 reward =  231.0\n",
      "\n",
      " episode =  205 reward =  165.0\n",
      "\n",
      " episode =  206 reward =  392.0\n",
      "\n",
      " episode =  207 reward =  166.0\n",
      "\n",
      " episode =  208 reward =  327.0\n",
      "\n",
      " episode =  209 reward =  213.0\n",
      "\n",
      " episode =  210 reward =  326.0\n",
      "\n",
      " episode =  211 reward =  260.0\n",
      "\n",
      " episode =  212 reward =  226.0\n",
      "\n",
      " episode =  213 reward =  425.0\n",
      "\n",
      " episode =  214 reward =  279.0\n",
      "\n",
      " episode =  215 reward =  447.0\n",
      "\n",
      " episode =  216 reward =  420.0\n",
      "\n",
      " episode =  217 reward =  577.0\n",
      "\n",
      " episode =  218 reward =  211.0\n",
      "\n",
      " episode =  219 reward =  351.0\n",
      "\n",
      " episode =  220 reward =  351.0\n",
      "\n",
      " episode =  221 reward =  671.0\n",
      "\n",
      " episode =  222 reward =  334.0\n",
      "\n",
      " episode =  223 reward =  231.0\n",
      "\n",
      " episode =  224 reward =  179.0\n",
      "\n",
      " episode =  225 reward =  305.0\n",
      "\n",
      " episode =  226 reward =  276.0\n",
      "\n",
      " episode =  227 reward =  319.0\n",
      "\n",
      " episode =  228 reward =  443.0\n",
      "\n",
      " episode =  229 reward =  367.0\n",
      "\n",
      " episode =  230 reward =  721.0\n",
      "\n",
      " episode =  231 reward =  225.0\n",
      "\n",
      " episode =  232 reward =  780.0\n",
      "\n",
      " episode =  233 reward =  208.0\n",
      "\n",
      " episode =  234 reward =  418.0\n",
      "\n",
      " episode =  235 reward =  275.0\n",
      "\n",
      " episode =  236 reward =  198.0\n",
      "\n",
      " episode =  237 reward =  206.0\n",
      "\n",
      " episode =  238 reward =  362.0\n",
      "\n",
      " episode =  239 reward =  332.0\n",
      "\n",
      " episode =  240 reward =  340.0\n",
      "\n",
      " episode =  241 reward =  1904.0\n",
      "\n",
      " episode =  242 reward =  802.0\n",
      "\n",
      " episode =  243 reward =  1815.0\n",
      "\n",
      " episode =  244 reward =  687.0\n",
      "\n",
      " episode =  245 reward =  457.0\n",
      "\n",
      " episode =  246 reward =  202.0\n",
      "\n",
      " episode =  247 reward =  206.0\n",
      "\n",
      " episode =  248 reward =  230.0\n",
      "\n",
      " episode =  249 reward =  426.0\n",
      "\n",
      " episode =  250 reward =  181.0\n",
      "\n",
      " episode =  251 reward =  167.0\n",
      "\n",
      " episode =  252 reward =  206.0\n",
      "\n",
      " episode =  253 reward =  149.0\n",
      "\n",
      " episode =  254 reward =  120.0\n",
      "\n",
      " episode =  255 reward =  206.0\n",
      "\n",
      " episode =  256 reward =  161.0\n",
      "\n",
      " episode =  257 reward =  153.0\n",
      "\n",
      " episode =  258 reward =  127.0\n",
      "\n",
      " episode =  259 reward =  167.0\n",
      "\n",
      " episode =  260 reward =  131.0\n",
      "\n",
      " episode =  261 reward =  185.0\n",
      "\n",
      " episode =  262 reward =  303.0\n",
      "\n",
      " episode =  263 reward =  179.0\n",
      "\n",
      " episode =  264 reward =  235.0\n",
      "\n",
      " episode =  265 reward =  237.0\n",
      "\n",
      " episode =  266 reward =  223.0\n",
      "\n",
      " episode =  267 reward =  131.0\n",
      "\n",
      " episode =  268 reward =  164.0\n",
      "\n",
      " episode =  269 reward =  287.0\n",
      "\n",
      " episode =  270 reward =  393.0\n",
      "\n",
      " episode =  271 reward =  509.0\n",
      "\n",
      " episode =  272 reward =  685.0\n",
      "\n",
      " episode =  273 reward =  574.0\n",
      "\n",
      " episode =  274 reward =  561.0\n",
      "\n",
      " episode =  275 reward =  478.0\n",
      "\n",
      " episode =  276 reward =  871.0\n",
      "\n",
      " episode =  277 reward =  478.0\n",
      "\n",
      " episode =  278 reward =  723.0\n",
      "\n",
      " episode =  279 reward =  934.0\n",
      "\n",
      " episode =  280 reward =  766.0\n",
      "\n",
      " episode =  281 reward =  656.0\n",
      "\n",
      " episode =  282 reward =  10220.0\n",
      "\n",
      " episode =  283 reward =  514.0\n",
      "\n",
      " episode =  284 reward =  464.0\n",
      "\n",
      " episode =  285 reward =  864.0\n",
      "\n",
      " episode =  286 reward =  188.0\n",
      "\n",
      " episode =  287 reward =  1328.0\n",
      "\n",
      " episode =  288 reward =  352.0\n",
      "\n",
      " episode =  289 reward =  290.0\n",
      "\n",
      " episode =  290 reward =  397.0\n",
      "\n",
      " episode =  291 reward =  475.0\n",
      "\n",
      " episode =  292 reward =  499.0\n",
      "\n",
      " episode =  293 reward =  422.0\n",
      "\n",
      " episode =  294 reward =  438.0\n",
      "\n",
      " episode =  295 reward =  538.0\n",
      "\n",
      " episode =  296 reward =  761.0\n",
      "\n",
      " episode =  297 reward =  714.0\n",
      "\n",
      " episode =  298 reward =  278.0\n",
      "\n",
      " episode =  299 reward =  390.0\n",
      "\n",
      " episode =  300 reward =  401.0\n",
      "\n",
      " episode =  301 reward =  1072.0\n",
      "\n",
      " episode =  302 reward =  732.0\n",
      "\n",
      " episode =  303 reward =  819.0\n",
      "\n",
      " episode =  304 reward =  398.0\n",
      "\n",
      " episode =  305 reward =  593.0\n",
      "\n",
      " episode =  306 reward =  236.0\n",
      "\n",
      " episode =  307 reward =  656.0\n",
      "\n",
      " episode =  308 reward =  1179.0\n",
      "\n",
      " episode =  309 reward =  2667.0\n",
      "\n",
      " episode =  310 reward =  418.0\n",
      "\n",
      " episode =  311 reward =  482.0\n",
      "\n",
      " episode =  312 reward =  571.0\n",
      "\n",
      " episode =  313 reward =  404.0\n",
      "\n",
      " episode =  314 reward =  336.0\n",
      "\n",
      " episode =  315 reward =  1405.0\n",
      "\n",
      " episode =  316 reward =  506.0\n",
      "\n",
      " episode =  317 reward =  2011.0\n",
      "\n",
      " episode =  318 reward =  708.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ENV=\"CartPole-v1\"\n",
    "\n",
    "N_EPISODES=500\n",
    "\n",
    "# set the env\n",
    "env=gym.make(ENV) # env to import\n",
    "env.reset() # reset to env\n",
    "\n",
    "\n",
    "Agent = REINFORCE(env)\n",
    "\n",
    "Agent.train(N_EPISODES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
