{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 05 - Policy Gradients - Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement REINFORCE, a policy gradient method based on Monte Carlo sampling.\n",
    "The key idea underlying policy gradients is to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to lower return, until you arrive at the optimal policy.\n",
    "\n",
    "The REINFORCE algorithm comprises the following steps:\n",
    "0. Initialize and reset the environment.\n",
    "1. Get the state from the environment.\n",
    "2. Feed forward our policy network to predict the probability of each action we should take. We’ll sample from this distribution to choose which action to take (i.e. toss a biased coin). This implies that the ouput layer of the neural network has a Softmax activation function.\n",
    "3. Receive the reward and the next state state from the environment for the action we took.\n",
    "4. Store this transition sequence of state, action, reward, for later training.\n",
    "5. Repeat steps 1–4. If we receive the done flag from the game it means the episode is over.\n",
    "6. Once the episode is over, we train our neural network to learn from our stored transitions using our gradient update rule. After training you can clear the stored states, actions and rewards from the memory. \n",
    "7. Play next episode and repeat steps above until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. The cartpole environment\n",
    "\n",
    "- Solve the OpenAI Gym cartpole environment with REINFORCE. \n",
    "- Plot the reward history (learning curve). For each episode plot the cumulative reward, or even better: plot average cumulative reward over a certain number of episodes (for example 50 episodes).\n",
    "- What is the effect of alpha, the learning rate for the gradient?\n",
    "- Do hyperparamter tuning to increase the speed of learning.\n",
    "- Compare the results to the ones of Deep Q-learning. Check the speed of learning, consistency of the results, etc.\n",
    "- Explain how the agent will explore a lot in the beginning and gradually will exploit more and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of REINFORCE for the Cartpole environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lunarlander environment\n",
    "\n",
    "- Solve the OpenAI Gym Lunarlander environment with REINFORCE. \n",
    "- Plot the reward history (learning curve). For each episode plot the cumulative reward, or even better: plot average cumulative reward over a certain number of episodes (for example 50 episodes).\n",
    "- Compare the results to the ones of Deep Q-learning. Check the speed of learning, consistency of the results, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of REINFORCE for the Lunarlander environment \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CarRacing environment ---- OPTIONAL ----\n",
    "\n",
    "- Solve the OpenAI Gym CarRacing environment with REINFORCE. \n",
    "- Plot the reward history (learning curve). For each episode plot the cumulative reward, or even better: plot average cumulative reward over a certain number of episodes (for example 50 episodes).\n",
    "- Compare the results to the ones of Deep Q-learning. Check the speed of learning, consistency of the results, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class REINFORCE:\n",
    "  def __init__(self, env, path=None):\n",
    "    self.env=env \n",
    "    self.state_shape=env.observation_space.shape # the state space\n",
    "    self.action_shape=env.action_space.n # the action space\n",
    "    self.gamma=0.99 # decay rate of past observations\n",
    "    self.alpha=1e-4 # learning rate of gradient\n",
    "    self.learning_rate=0.01 # learning of deep learning model\n",
    "    \n",
    "    if not path:\n",
    "      self.model=self.build_policy_network() #build model\n",
    "    else:\n",
    "      self.model=self.load_model(path) #import model\n",
    "\n",
    "    # record observations\n",
    "    self.states=[]\n",
    "    self.gradients=[] \n",
    "    self.rewards=[]\n",
    "    self.probs=[]\n",
    "    self.discounted_rewards=[]\n",
    "    self.total_rewards=[]\n",
    "    \n",
    "    \n",
    "\n",
    "  def build_policy_network(self):\n",
    "    \n",
    "\n",
    "    # BUILD MODEL ####################################################################\n",
    "        \n",
    "    return model\n",
    "\n",
    "  def hot_encode_action(self, action):\n",
    "\n",
    "    action_encoded=np.zeros(self.action_shape)\n",
    "    action_encoded[action]=1\n",
    "\n",
    "    return action_encoded\n",
    "  \n",
    "  def remember(self, state, action, action_prob, reward):\n",
    "    \n",
    "    # STORE EACH STATE, ACTION AND REWARD into the episodic momory #############################\n",
    "\n",
    "\n",
    "  def compute_action(self, state):\n",
    "\n",
    "\n",
    "    # COMPUTE THE ACTION FROM THE SOFTMAX PROBABILITIES\n",
    "\n",
    "    return action, action_probability_distribution\n",
    "\n",
    "\n",
    "  def get_discounted_rewards(self, rewards): \n",
    "   \n",
    "    discounted_rewards=[]\n",
    "    cumulative_total_return=0\n",
    "    # iterate the rewards backwards and and calc the total return \n",
    "    for reward in rewards[::-1]:      \n",
    "      cumulative_total_return=(cumulative_total_return*self.gamma)+reward\n",
    "      discounted_rewards.insert(0, cumulative_total_return)\n",
    "\n",
    "    # normalize discounted rewards\n",
    "    mean_rewards=np.mean(discounted_rewards)\n",
    "    std_rewards=np.std(discounted_rewards)\n",
    "    norm_discounted_rewards=(discounted_rewards-\n",
    "                          mean_rewards)/(std_rewards+1e-7) # avoiding zero div\n",
    "    \n",
    "    return norm_discounted_rewards\n",
    "\n",
    "  def train_policy_network(self):\n",
    "       \n",
    "    # get X_train\n",
    "    states=np.vstack(self.states)\n",
    "\n",
    "    # get y_train\n",
    "    gradients=np.vstack(self.gradients)\n",
    "    rewards=np.vstack(self.rewards)\n",
    "    discounted_rewards=self.get_discounted_rewards(rewards)\n",
    "    gradients*=discounted_rewards\n",
    "    gradients=self.alpha*np.vstack([gradients])+self.probs\n",
    "    y_train = gradients\n",
    "    history=self.model.train_on_batch(states, y_train)\n",
    "    \n",
    "    self.states, self.probs, self.gradients, self.rewards=[], [], [], []\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "  def train(self, episodes):\n",
    "     \n",
    "\n",
    "\n",
    "  def hot_encode_action(self, action):\n",
    "\n",
    "    action_encoded=np.zeros(self.action_shape)\n",
    "    action_encoded[action]=1\n",
    "\n",
    "    return action_encoded\n",
    "  \n",
    " \n",
    "\n",
    "ENV=\"CartPole-v1\"\n",
    "\n",
    "N_EPISODES=500\n",
    "\n",
    "\n",
    "\n",
    "# set the env\n",
    "env=gym.make(ENV) # env to import\n",
    "env.reset() # reset to env \n",
    "\n",
    "Agent = REINFORCE(env)\n",
    "\n",
    "Agent.train(N_EPISODES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
